{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion-MNIST-based simulated production line prediction\n",
    "<font color=#FF0000>Description, TO BE DONE!!</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# necessary packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# additional certain short functions\n",
    "from torch import is_tensor\n",
    "from matplotlib.pyplot import pause\n",
    "from random import randint\n",
    "from copy import deepcopy\n",
    "from math import floor, ceil\n",
    "from torch import stack\n",
    "from torch import cat\n",
    "\n",
    "# import custom functions\n",
    "from data_generate import *\n",
    "from data_io import *\n",
    "from CNN import *\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize dataset\n",
    "class ProductLineDataset(Dataset):\n",
    "    def __init__(self, image: torch.Tensor, gt: torch.Tensor):\n",
    "        self.x = image.reshape(len(image), 1, 28*3, 28*4).type(torch.float32)\n",
    "        self.y = gt.type(torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file names\n",
    "CSV_NAME_TRAIN_02346 = 'train_02346.csv'\n",
    "CSV_NAME_TEST_02346 = 'test_02346.csv'\n",
    "CSV_NAME_TEST_579 = 'test_579.csv'\n",
    "CSV_NAME_TEST_18 = 'test_18.csv'\n",
    "IMAGE_NAME_TRAIN_02346 = 'train_02346'\n",
    "IMAGE_NAME_TEST_02346 = 'test_02346'\n",
    "IMAGE_NAME_TEST_579 = 'test_579'\n",
    "IMAGE_NAME_TEST_18 = 'test_18'\n",
    "\n",
    "# import data\n",
    "gt_train_02346 = gt_import(CSV_NAME_TRAIN_02346)\n",
    "gt_test_02346 = gt_import(CSV_NAME_TEST_02346)\n",
    "gt_test_579 = gt_import(CSV_NAME_TEST_579)\n",
    "gt_test_18 = gt_import(CSV_NAME_TEST_18)\n",
    "image_train_02346 = image_import(gt_train_02346, IMAGE_NAME_TRAIN_02346)/255.\n",
    "image_test_02346 = image_import(gt_test_02346, IMAGE_NAME_TEST_02346)/255.\n",
    "image_test_579 = image_import(gt_test_579, IMAGE_NAME_TEST_579)/255.\n",
    "image_test_18 = image_import(gt_test_18, IMAGE_NAME_TEST_18)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "dataset_train_02346 = ProductLineDataset(image_train_02346, gt_train_02346)\n",
    "dataset_test_02346 = ProductLineDataset(image_test_02346, gt_test_02346)\n",
    "dataset_test_579 = ProductLineDataset(image_test_579, gt_test_579)\n",
    "dataset_test_18 = ProductLineDataset(image_test_18, gt_test_18)\n",
    "\n",
    "# define dataloader\n",
    "dataloader_train_02346 = DataLoader(dataset_train_02346, batch_size=64, shuffle=True)\n",
    "dataloader_test_02346 = DataLoader(dataset_test_02346, batch_size=1, shuffle=True)  # batch_size=1 for test\n",
    "dataloader_test_579 = DataLoader(dataset_test_579, batch_size=1, shuffle=True)\n",
    "dataloader_test_18 = DataLoader(dataset_test_18, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 84, 112])\n",
      "Shape of y:  torch.Size([64])\n",
      "Data type of y:  torch.float32\n",
      "tensor([6., 3., 7., 6., 6., 6., 4., 3., 4., 3., 7., 3., 6., 4., 3., 6., 4., 7.,\n",
      "        5., 3., 6., 6., 3., 4., 6., 4., 3., 3., 3., 4., 4., 7., 6., 6., 6., 4.,\n",
      "        6., 5., 7., 4., 5., 3., 3., 4., 7., 6., 5., 3., 4., 5., 3., 3., 4., 3.,\n",
      "        7., 3., 3., 5., 3., 7., 3., 3., 7., 6.])\n"
     ]
    }
   ],
   "source": [
    "# visualize the shape\n",
    "for X, y in dataloader_train_02346:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape)\n",
    "    print(\"Data type of y: \", y.dtype)\n",
    "    print(y)    # need to transfer [64] to [64, 10] by one-hot coding\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# apply CUDA\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear((28*3)*(28*4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        score = self.linear(x)\n",
    "        return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions \n",
    "def train(model, dataloader, loss_fn, optimizer):   # put epoch in main better for loss calculation\n",
    "    # size of dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # set model mode\n",
    "    model.train()\n",
    "    \n",
    "    # train batches per epoch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # move data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        score = model(X)\n",
    "        loss = loss_fn(score, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 40 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(model, dataloader, loss_fn_train, loss_fn_test):\n",
    "    # size of dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # set model mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            # move data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            score = model(X)\n",
    "            test_loss += loss_fn_train(score, y).item()\n",
    "            correct += (y.item() - loss_fn_test(score, y).item()) / (y.item())    # only for batch_size=1\n",
    "    \n",
    "    # calculate the average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= num_batches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup and train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=9408, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01)\n",
      "    (11): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basic_model = NeuralNetwork().to(device=device)\n",
    "print(basic_model)\n",
    "\n",
    "# define parameters\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# define lose function\n",
    "loss_fn_train = nn.MSELoss()\n",
    "loss_fn_test = nn.L1Loss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(basic_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epoch\n",
    "epoch = 10\n",
    "\n",
    "# start training\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(basic_model, dataloader_train_02346, loss_fn_train, optimizer)\n",
    "    test(basic_model, dataloader_test_02346, loss_fn_train, loss_fn_test)\n",
    "print(\"Done!!\")\n",
    "\n",
    "# model saving\n",
    "torch.save(basic_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8960, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc4): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_channel = 1\n",
    "channel_1 = 16\n",
    "channel_2 = 32\n",
    "channel_3 = 64\n",
    "node_1 = 1024\n",
    "node_2 = 256\n",
    "node_3 = 64\n",
    "node_4 = 16\n",
    "out_channel = 1\n",
    "\n",
    "CNN_model = CNN(in_channel, channel_1, channel_2, channel_3, node_1, node_2, \\\n",
    "    node_3, node_4, out_channel).to(device=device)\n",
    "print(CNN_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 23.795898  [    0/10575]\n",
      "loss: 24.704681  [ 2560/10575]\n",
      "loss: 20.680552  [ 5120/10575]\n",
      "loss: 22.586029  [ 7680/10575]\n",
      "loss: 23.913736  [10240/10575]\n",
      "Test Error: \n",
      " Accuracy: -2.8%, Avg loss: 22.694621 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 23.663610  [    0/10575]\n",
      "loss: 25.107864  [ 2560/10575]\n",
      "loss: 24.482382  [ 5120/10575]\n",
      "loss: 26.349979  [ 7680/10575]\n",
      "loss: 22.797228  [10240/10575]\n",
      "Test Error: \n",
      " Accuracy: -2.8%, Avg loss: 22.694743 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 27.803116  [    0/10575]\n",
      "loss: 22.819584  [ 2560/10575]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train(CNN_model, dataloader_train_02346, loss_fn_train, optimizer)\n\u001b[0;32m      8\u001b[0m     test(CNN_model, dataloader_test_02346, loss_fn_train, loss_fn_test)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [8], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     11\u001b[0m     \u001b[39m# move data to device\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 14\u001b[0m     score \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(score, y)\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\OneDrive - Amgen\\Documents\\BB_prediction\\CNN.py:55\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 55\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m     56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[0;32m     57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\shuang07\\.conda\\envs\\auto_labeling\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set epoch\n",
    "epoch = 5\n",
    "\n",
    "# start training\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(CNN_model, dataloader_train_02346, loss_fn_train, optimizer)\n",
    "    test(CNN_model, dataloader_test_02346, loss_fn_train, loss_fn_test)\n",
    "print(\"Done!!\")\n",
    "\n",
    "# model saving\n",
    "torch.save(CNN_model.state_dict(), \"CNN.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#FF0000>Visualization reference (non-used yet) <font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single bounding box testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(col, row, box_x, box_y):    # position + length / width (visualization usage)\n",
    "    view = torch.zeros([28*3, 28*4], dtype=torch.float32)\n",
    "    # set the box, outside the target with one pixel\n",
    "    view[row-box_y:row+1, col-box_x] = 1\n",
    "    view[row-box_y:row+1, col] = 1\n",
    "    view[row-box_y, col-box_x:col+1] = 1\n",
    "    view[row, col-box_x:col+1] = 1\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import pause\n",
    "from random import randint\n",
    "from copy import deepcopy\n",
    "\n",
    "rand_x = randint(14, 27)\n",
    "rand_y = randint(27, 28*3-1)\n",
    "print(\"new start position [x, y] = [\", rand_x,\",\", rand_y, \"]\")\n",
    "\n",
    "rand_start = deepcopy(aug_sample)\n",
    "box_origin = box(27, 21, 27, 15)    # array for visualization, the only manual input part\n",
    "box_sample = deepcopy(box_origin)\n",
    "\n",
    "plt.imshow(rand_start, cmap='gray')\n",
    "plt.imshow(box_origin, cmap='gray', alpha=0.3)\n",
    "pause(0.1)\n",
    "\n",
    "# initial movement\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        rand_start[rand_y-i][rand_x-j] = rand_start[27-i][27-j]\n",
    "\n",
    "for i in range(29):\n",
    "    for j in range(29):\n",
    "        box_origin[rand_y-i+1][rand_x-j+1] = box_origin[28-i][28-j]\n",
    "\n",
    "# clean other part\n",
    "rand_start[:rand_y-28+1, :] = 0\n",
    "rand_start[:, rand_x+1:rand_x+28+1] = 0\n",
    "box_origin[:rand_y-28, :] = 0       # 1-pixel cleaning region difference\n",
    "box_origin[:, rand_x+1:rand_x+28+1] = 0\n",
    "\n",
    "plt.imshow(rand_start, cmap='gray')\n",
    "plt.imshow(box_origin, cmap='gray', alpha=0.3)\n",
    "pause(0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sequential bounding box visualization and random position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "from torch import cat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import pause\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential box preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "stride = 5\n",
    "first_frame = deepcopy(box_sample)\n",
    "next_frame = deepcopy(box_sample)\n",
    "y_box = deepcopy(box_sample)\n",
    "\n",
    "# show info\n",
    "print(y_box.size())\n",
    "plt.imshow(y_box, cmap='gray')\n",
    "pause(0.1)\n",
    "\n",
    "for i in range(floor((112-28)/stride)):\n",
    "   # moving part (1-dim only)\n",
    "   for j in range(30):  # bigger range to cover bounding box\n",
    "      next_frame[:, 30+stride*(i+1)-(j+1)] = next_frame[:, 30+stride*i-(j+1)]\n",
    "   next_frame[:, :stride*(i+1)] = 0  # clean other area\n",
    "   \n",
    "   # sequencing part\n",
    "   if i == 0:\n",
    "      y_box = stack((first_frame, next_frame)) \n",
    "   else:\n",
    "      y_box = cat((y_box, next_frame.reshape(1, 28*3, 28*4)), dim=0)\n",
    "      \n",
    "   # # show info\n",
    "   # print(y_box.size())\n",
    "   # plt.imshow(y_box[i+1], cmap='gray')\n",
    "   # pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallel moving and concat last frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential data preparation\n",
    "y_box_moved = deepcopy(y_box)\n",
    "y_moved = deepcopy(y)\n",
    "rand_x = randint(14, 27)\n",
    "rand_y = randint(27, 28*3-1)\n",
    "print(\"new start position [x, y] = [\", rand_x,\",\", rand_y, \"]\")\n",
    "\n",
    "# # visualization for no parallel movement\n",
    "# for i in range(len(y_box_moved)):\n",
    "#     plt.imshow(y_moved[i], cmap='gray')\n",
    "#     plt.imshow(y_box_moved[i], cmap='gray', alpha=0.3)\n",
    "#     pause(0.1)\n",
    "\n",
    "# all frames move left parellel\n",
    "for i in range(floor((112-28)/stride)+1): \n",
    "    # moving items\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            y_moved[i][rand_y-j][rand_x+stride*i-k] = y_moved[i][27-j][27+stride*i-k]\n",
    "    # moving box\n",
    "    for j in range(29):\n",
    "        for k in range(29):\n",
    "            y_box_moved[i][rand_y-j+1][rand_x+stride*i-k+1] = y_box_moved[i][28-j][28+stride*i-k]\n",
    "          \n",
    "    # clean other area\n",
    "    y_moved[i][:rand_y-28+1, :] = 0\n",
    "    y_moved[i][:, rand_x+stride*i+1:] = 0\n",
    "    y_box_moved[i][:rand_y-28+1, :] = 0\n",
    "    y_box_moved[i][:, rand_x+stride*i+1:] = 0\n",
    "   \n",
    "    # show info\n",
    "    plt.imshow(y_moved[i], cmap='gray')\n",
    "    plt.imshow(y_box_moved[i], cmap='gray', alpha=0.3)\n",
    "    pause(0.1)\n",
    "\n",
    "# initialization\n",
    "frame_old = len(y_moved)   # number of old frames\n",
    "next_frame = deepcopy(y_moved[frame_old-1])\n",
    "next_frame_box = deepcopy(y_box_moved[frame_old-1])\n",
    "\n",
    "# last frame moves right and cat\n",
    "for i in range(ceil((28-rand_x)/stride)+1):\n",
    "    # move next frame\n",
    "    for j in range(28):\n",
    "        if rand_x+stride*(frame_old+i)-j < 112:\n",
    "           next_frame[:, rand_x+stride*(frame_old+i)-j] = next_frame[:, rand_x+stride*(frame_old+i-1)-j]\n",
    "    for j in range(29):\n",
    "        if rand_x+stride*(frame_old+i)-j+1 < 112:\n",
    "           next_frame_box[:, rand_x+stride*(frame_old+i)-j+1] = next_frame_box[:, rand_x+stride*(frame_old+i-1)-j+1]\n",
    "    # clean and cat\n",
    "    next_frame[:, :rand_x+stride*(frame_old+i)-28+1] = 0\n",
    "    y_moved = cat((y_moved, next_frame.reshape(1, 28*3, 28*4)), dim=0)\n",
    "    next_frame_box[:, :rand_x+stride*(frame_old+i)-28+1] = 0\n",
    "    y_box_moved = cat((y_box_moved, next_frame_box.reshape(1, 28*3, 28*4)), dim=0)\n",
    "   \n",
    "    # show info\n",
    "    plt.imshow(y_moved[frame_old+i], cmap='gray')\n",
    "    plt.imshow(y_box_moved[frame_old+i], cmap='gray', alpha=0.3)\n",
    "    pause(0.1)\n",
    "\n",
    "print(y_moved.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('auto_labeling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec129e0590d45a22483b850cd1c473456f2c02a48b73be1599d608a53736fee7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
